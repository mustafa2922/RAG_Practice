import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os

load_dotenv()

# =====================================================
# Page Configuration
st.set_page_config(
    page_title="AI - Fatwa Chatbot",
    page_icon="üïå",
    layout="wide"
)

# =====================================================
# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# =====================================================
# Load vectorstore (cached for performance)
@st.cache_resource
def load_vectorstore():
    """Load the FAISS vectorstore - runs only once"""
    embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
    vectorstore = FAISS.load_local(
        "../fatwa_index", 
        embedding_model,
        allow_dangerous_deserialization=True
    )
    return vectorstore

@st.cache_resource
def load_llm():
    """Load the LLM - runs only once"""
    llm = ChatGroq(
        model="openai/gpt-oss-120b",
        temperature=0.1,
        api_key=os.getenv("GROQ_API_KEY")
    )
    return llm

def create_qa_chain(vectorstore, llm, k=3):
    """Create conversational retrieval chain with memory"""
    
    # Custom Prompt Template
    prompt_template = """
ÿ¢Ÿæ ÿß€å⁄© ÿßÿ≥ŸÑÿßŸÖ€å ÿπÿßŸÑŸÖ ÿßŸàÿ± ŸÖŸÅÿ™€å €Å€å⁄∫ ÿ¨Ÿà ÿµÿ±ŸÅ ÿØ€å€í ⁄Øÿ¶€í ŸÅÿ™ÿßŸà€å ⁄©€å ÿ®ŸÜ€åÿßÿØ Ÿæÿ± ÿ¨Ÿàÿßÿ® ÿØ€åÿ™€í €Å€å⁄∫€î

Ÿæ⁄Ü⁄æŸÑ€å ⁄ØŸÅÿ™⁄ØŸà (Chat History):
{chat_history}

ŸÅÿ™ÿßŸà€å (Context):
{context}

ÿ≥ŸàÿßŸÑ (Question): {question}

€ÅÿØÿß€åÿßÿ™ (Instructions):
1. ÿß⁄Øÿ± ÿ≥ŸàÿßŸÑ Ÿæ⁄Ü⁄æŸÑ€í ÿ≥ŸàÿßŸÑÿßÿ™ ÿ≥€í ŸÖÿ™ÿπŸÑŸÇ €Å€í ÿ™Ÿà ⁄ØŸÅÿ™⁄ØŸà ⁄©ÿß ÿ™ÿ≥ŸÑÿ≥ŸÑ ÿ®ÿ±ŸÇÿ±ÿßÿ± ÿ±⁄©⁄æ€å⁄∫€î
2. ÿ™ŸÖÿßŸÖ ŸÅÿ™ÿßŸà€å ⁄©Ÿà ÿ®ÿ∫Ÿàÿ± Ÿæ⁄ë⁄æ€å⁄∫ ÿßŸàÿ± ÿßŸÜ€Å€å ⁄©€í ÿßŸÜÿØÿ± ŸÖŸàÿ¨ŸàÿØ ŸÖÿπŸÑŸàŸÖÿßÿ™ ⁄©€å ÿ®ŸÜ€åÿßÿØ Ÿæÿ± ÿ¨Ÿàÿßÿ® ÿ™€åÿßÿ± ⁄©ÿ±€å⁄∫€î  
3. ÿ¨Ÿàÿßÿ® **ÿ≥ÿßÿÆÿ™€Å ÿßŸÜÿØÿßÿ≤ (structured format)** ŸÖ€å⁄∫ ÿØ€å⁄∫ÿå ÿØÿ±ÿ¨ ÿ∞€åŸÑ ÿ≠ÿµŸà⁄∫ ⁄©€í ÿ≥ÿßÿ™⁄æ:
   - **ÿÆŸÑÿßÿµ€Å (Summary):** ŸÅÿ™ÿßŸà€å ⁄©€í ŸÖÿ∑ÿßÿ®ŸÇ ŸÖÿÆÿ™ÿµÿ± ŸÜÿ™€åÿ¨€Å€î  
   - **ÿ™ŸÅÿµ€åŸÑ (Explanation):** ŸÅÿ™ÿßŸà€å ŸÖ€å⁄∫ ÿ®€åÿßŸÜ ⁄©ÿ±ÿØ€Å ÿßÿµŸÑ ÿ™ŸÅÿµ€åŸÑ€å ÿ®ÿßÿ™€î  
   - **ÿØŸÑÿßÿ¶ŸÑ €åÿß ÿ≠ŸàÿßŸÑ€í (References):** Ÿà€Å€å ÿ¢€åÿßÿ™ÿå ÿßÿ≠ÿßÿØ€åÿ´ €åÿß ŸÅŸÇ€Å€å ŸÖÿßÿÆÿ∞ ÿ¨Ÿà ŸÅÿ™ÿßŸà€å ŸÖ€å⁄∫ ÿ∞⁄©ÿ± €ÅŸà⁄∫€î  
   - **ŸÜÿ™€åÿ¨€Å (Conclusion):** ŸÅÿ™ÿßŸà€å ⁄©€í ŸÖÿ∑ÿßÿ®ŸÇ ÿ≠ÿ™ŸÖ€å ÿ≠⁄©ŸÖ €åÿß ŸÅ€åÿµŸÑ€Å€î  
4. ŸÅÿ™ÿßŸà€å ⁄©€í ÿßŸÑŸÅÿßÿ∏ ⁄©Ÿà ŸÖŸÖ⁄©ŸÜ ÿ≠ÿØ ÿ™⁄© ÿ®ÿ±ŸÇÿ±ÿßÿ± ÿ±⁄©⁄æ€å⁄∫ÿõ ÿµÿ±ŸÅ ŸÖÿπŸÖŸàŸÑ€å ŸÜÿ≠Ÿà€å ÿ™ÿ®ÿØ€åŸÑ€å ⁄©ÿ±€å⁄∫ ÿß⁄Øÿ± ÿ¨ŸÖŸÑ€Å ŸÜÿßŸÇÿµ €ÅŸà€î  
5. ÿßŸæŸÜ€å ÿ∑ÿ±ŸÅ ÿ≥€í ⁄©Ÿàÿ¶€å ŸÜÿ¶€å ÿ®ÿßÿ™ÿå Ÿàÿ∂ÿßÿ≠ÿ™ÿå ÿ™ÿ¥ÿ±€åÿ≠ÿå €åÿß ŸÖÿ´ÿßŸÑ **ÿ¥ÿßŸÖŸÑ ŸÜ€Å ⁄©ÿ±€å⁄∫€î**  
6. ÿß⁄Øÿ± ÿ≥ŸàÿßŸÑ ⁄©ÿß ŸÖ⁄©ŸÖŸÑ ÿ¨Ÿàÿßÿ® ŸÅÿ™ÿßŸà€å ŸÖ€å⁄∫ ŸÜ€Å €ÅŸà ÿ™Ÿà Ÿàÿßÿ∂ÿ≠ ÿ∑Ÿàÿ± Ÿæÿ± ŸÑ⁄©⁄æ€å⁄∫:
   **"ŸÖÿπÿ∞ÿ±ÿ™ÿå ÿßÿ≥ ÿ≥ŸàÿßŸÑ ⁄©ÿß ŸÖ⁄©ŸÖŸÑ ÿ¨Ÿàÿßÿ® ŸÖŸàÿ¨ŸàÿØ€Å ŸÅÿ™ÿßŸà€å ŸÖ€å⁄∫ ŸÜ€Å€å⁄∫ ŸÖŸÑÿß€î"**  
7. ÿß⁄Øÿ± ⁄©ÿ≥€å ŸÅÿ™Ÿà€í ŸÖ€å⁄∫ ÿ™ÿ∂ÿßÿØ €åÿß ÿßÿÆÿ™ŸÑÿßŸÅ €ÅŸà ÿ™Ÿà ÿµÿ±ŸÅ Ÿà€Å€å ŸÑ⁄©⁄æ€å⁄∫ ÿ¨Ÿà ŸÅÿ™ÿßŸà€å ŸÖ€å⁄∫ ŸÖÿ∞⁄©Ÿàÿ± €Å€íÿõ ÿßŸæŸÜÿß ŸÅ€åÿµŸÑ€Å ŸÜ€Å ÿØ€å⁄∫€î  
8. ÿßÿØÿ® ÿßŸàÿ± ÿ≥ÿßÿØ⁄Ø€å ⁄©€í ÿ≥ÿßÿ™⁄æ ÿßÿ±ÿØŸà ŸÖ€å⁄∫ ŸÑ⁄©⁄æ€å⁄∫€î

ÿ¨Ÿàÿßÿ® (Answer):
"""
    
    CONDENSE_QUESTION_PROMPT = PromptTemplate(
        template="""Ÿæ⁄Ü⁄æŸÑ€å ⁄ØŸÅÿ™⁄ØŸà ÿßŸàÿ± ŸÜ€åÿß ÿ≥ŸàÿßŸÑ ÿØ€å⁄©⁄æ ⁄©ÿ±ÿå ÿß€å⁄© ŸÖ⁄©ŸÖŸÑ ÿßŸàÿ± Ÿàÿßÿ∂ÿ≠ standalone ÿ≥ŸàÿßŸÑ ÿ®ŸÜÿßÿ¶€å⁄∫ ÿ¨Ÿà ÿ™ŸÖÿßŸÖ ÿ∂ÿ±Ÿàÿ±€å ÿ≥€åÿßŸÇ Ÿà ÿ≥ÿ®ÿßŸÇ ⁄©Ÿà ÿ¥ÿßŸÖŸÑ ⁄©ÿ±€í€î €å€Å ÿ≥ŸàÿßŸÑ ŸÖ⁄©ŸÖŸÑ ÿ∑Ÿàÿ± Ÿæÿ± ÿÆŸàÿØ ⁄©ŸÅ€åŸÑ €ÅŸàŸÜÿß ⁄Üÿß€Å€å€í ÿ™ÿß⁄©€Å ÿ®ÿ∫€åÿ± Ÿæ⁄Ü⁄æŸÑ€å ⁄ØŸÅÿ™⁄ØŸà ÿØ€å⁄©⁄æ€í ÿ®⁄æ€å ÿ≥ŸÖÿ¨⁄æ ÿ¢ ÿ≥⁄©€í€î

ŸÖÿ´ÿßŸÑ:
ÿß⁄Øÿ± ŸÜ€åÿß ÿ≥ŸàÿßŸÑ €Å€í: "ÿßŸàÿ± ÿßÿ≥ ⁄©€å ÿ≥ŸÜÿ™€å⁄∫ ⁄©€åÿß €Å€å⁄∫ÿü"
ÿßŸàÿ± Ÿæ⁄Ü⁄æŸÑÿß ÿ≥ŸàÿßŸÑ ŸÜŸÖÿßÿ≤ ⁄©€í ÿ®ÿßÿ±€í ŸÖ€å⁄∫ ÿ™⁄æÿß
ÿ™Ÿà standalone ÿ≥ŸàÿßŸÑ €ÅŸà ⁄Øÿß: "ŸÜŸÖÿßÿ≤ ⁄©€å ÿ≥ŸÜÿ™€å⁄∫ ⁄©€åÿß €Å€å⁄∫ÿü"

Ÿæ⁄Ü⁄æŸÑ€å ⁄ØŸÅÿ™⁄ØŸà:
{chat_history}

ŸÜ€åÿß ÿ≥ŸàÿßŸÑ: {question}

Standalone ÿ≥ŸàÿßŸÑ (ÿµÿ±ŸÅ ÿ≥ŸàÿßŸÑ ŸÑ⁄©⁄æ€å⁄∫ÿå ⁄©Ÿàÿ¶€å ÿßÿ∂ÿßŸÅ€å ÿ®ÿßÿ™ ŸÜ€Å€å⁄∫):""",
        input_variables=["chat_history", "question"]
    )
    
    QA_PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question", "chat_history"]
    )
    
    # Create retriever
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": k}
    )
    
    # Create memory
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    # Create conversational chain
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": QA_PROMPT},
        condense_question_prompt=CONDENSE_QUESTION_PROMPT,
        condense_question_llm=llm,  # Explicitly set LLM for question condensing
        return_generated_question=True,  # Return the condensed question for debugging
        verbose=False
    )
    
    return qa_chain

# =====================================================
# Main UI
st.title("üïå AI - Islamic Fatwa Chatbot")
st.markdown("### Get answers to your Islamic questions with conversation memory")
st.divider()

# Load resources
try:
    with st.spinner("Loading AI system..."):
        vectorstore = load_vectorstore()
        llm = load_llm()
    st.success("‚úÖ AI is ready!")
except Exception as e:
    st.error(f"‚ùå Error loading system: {str(e)}")
    st.info("üí° Make sure to set GROQ_API_KEY in your environment or use Streamlit secrets")
    st.stop()

# =====================================================
# Sidebar Controls
with st.sidebar:
    st.header("‚öôÔ∏è Settings")
    
    k = st.number_input(
        "üìä Fatwas to retrieve:",
        min_value=1,
        max_value=10,
        value=3,
        step=1,
        help="Number of relevant fatwas to use for context"
    )
    
    if st.button("üóëÔ∏è Clear Chat History", use_container_width=True):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.rerun()
    
    st.divider()
    
    st.header("‚ÑπÔ∏è About AI")
    st.markdown("""
    This AI chatbot uses:
    - **Conversational Memory**
    - **RAG (Retrieval Augmented Generation)**
    - **Authentic Fatwa Database**
    - **Advanced Language Model**
    
    **How it works:**
    1. Remembers previous questions and answers
    2. Retrieves relevant fatwas from database
    3. Provides contextual answers
    4. Shows source fatwas for verification
    """)
    
    st.divider()
    
    st.header("‚ö†Ô∏è Important Notes")
    st.warning("""
    - AI answers are based on the fatwa database
    - Always verify important matters with qualified scholars
    - Check the source fatwas provided
    - AI may refuse if information is insufficient
    """)
    
    st.divider()
    
    st.header("üí° Example Questions")
    examples = [
        "ŸÜŸÖÿßÿ≤ ŸÖ€å⁄∫ ÿ≥Ÿàÿ±€Å ŸÅÿßÿ™ÿ≠€Å Ÿæ⁄ë⁄æŸÜÿß ÿ∂ÿ±Ÿàÿ±€å €Å€íÿü",
        "ÿ±Ÿàÿ≤€í ⁄©€å ŸÜ€åÿ™ ⁄©ÿ® ÿ™⁄© ⁄©€å ÿ¨ÿß ÿ≥⁄©ÿ™€å €Å€íÿü",
        "ÿ≤⁄©ŸàŸ∞€É ⁄©€å ÿ¥ÿ±ÿ≠ ⁄©€åÿß €Å€íÿü",
        "Ÿàÿ∂Ÿà ⁄©€í ŸÅÿ±ÿßÿ¶ÿ∂ ⁄©€åÿß €Å€å⁄∫ÿü",
        "ÿ¨ŸÖÿπ€Å ⁄©€å ŸÜŸÖÿßÿ≤ ⁄©ÿ™ŸÜ€í ÿ±⁄©ÿπÿ™ €Å€íÿü"
    ]
    
    for example in examples:
        if st.button(example, use_container_width=True, key=f"example_{example[:10]}"):
            st.session_state.example_query = example
            st.rerun()

# =====================================================
# Display Chat History
st.markdown("## üí¨ Conversation")

# Display all previous messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Show sources if available
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("üìö View Source Fatwas", expanded=False):
                for i, doc in enumerate(message["sources"], 1):
                    st.markdown(f"**üìÑ Source #{i} - {doc.metadata.get('category', 'N/A')}**")
                    
                    url = doc.metadata.get('url', '#')
                    if url != '#':
                        st.markdown(f"üîó [View Original]({url})")
                    
                    st.text_area(
                        label="Content",
                        value=doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                        height=150,
                        key=f"msg_source_{message.get('timestamp', '')}_{i}",
                        label_visibility="collapsed"
                    )
                    st.divider()

# =====================================================
# Handle example queries from sidebar
if 'example_query' in st.session_state:
    user_input = st.session_state.example_query
    del st.session_state.example_query
else:
    user_input = None

# Chat input
if prompt := (user_input or st.chat_input("ÿßŸæŸÜÿß ÿ≥ŸàÿßŸÑ ŸæŸà⁄Ü⁄æ€å⁄∫ (Ask your question)...")):
    # Add user message to chat
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate response
    with st.chat_message("assistant"):
        with st.spinner("ü§î AI is thinking..."):
            try:
                # Create QA chain with current settings
                qa_chain = create_qa_chain(vectorstore, llm, k)
                
                # Load previous chat history into memory
                for msg in st.session_state.chat_history:
                    qa_chain.memory.chat_memory.add_user_message(msg["question"])
                    qa_chain.memory.chat_memory.add_ai_message(msg["answer"])
                
                # Get response
                result = qa_chain.invoke({"question": prompt})
                
                response = result['answer']
                source_docs = result['source_documents']
                
                # Debug: Show what question was actually used for retrieval
                if 'generated_question' in result:
                    with st.expander("üîç Debug: Question used for retrieval", expanded=False):
                        st.write(f"**Original question:** {prompt}")
                        st.write(f"**Condensed question:** {result['generated_question']}")
                
                # Display response
                st.markdown(response)
                
                # Show sources in expander
                with st.expander("üìö View Source Fatwas", expanded=False):
                    for i, doc in enumerate(source_docs, 1):
                        st.markdown(f"**üìÑ Source #{i} - {doc.metadata.get('category', 'N/A')}**")
                        
                        url = doc.metadata.get('url', '#')
                        if url != '#':
                            st.markdown(f"üîó [View Original]({url})")
                        
                        st.text_area(
                            label="Content",
                            value=doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                            height=150,
                            key=f"source_{i}",
                            label_visibility="collapsed"
                        )
                        st.divider()
                
                # Save to session state
                import time
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "sources": source_docs,
                    "timestamp": str(time.time())
                })
                
                # Update chat history for memory
                st.session_state.chat_history.append({
                    "question": prompt,
                    "answer": response
                })
                
            except Exception as e:
                error_msg = f"‚ùå Error generating response: {str(e)}"
                st.error(error_msg)
                st.info("üí° Make sure your GROQ_API_KEY is set correctly")
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })

# =====================================================
# Footer
st.divider()
st.markdown(
    """
    <div style='text-align: center; color: gray;'>
    üïå AI Chatbot System | Powered by RAG, FAISS & LLM with Memory<br>
    <small>For educational purposes - Always consult qualified scholars for important matters</small>
    </div>
    """,
    unsafe_allow_html=True
)